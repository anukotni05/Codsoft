!pip install torch torchvision transformers matplotlib
from google.colab import files
uploaded = files.upload()
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
from transformers import GPT2Tokenizer, GPT2LMHeadModel
def preprocess_image(image_path):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0)
    return image
resnet = models.resnet50(pretrained=True)
resnet = nn.Sequential(*list(resnet.children())[:-1])
resnet.eval()
def extract_features(image_path):
    image = preprocess_image(image_path)
    with torch.no_grad():
        features = resnet(image)
    features = features.squeeze().cpu().numpy()
    return features
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')
def generate_caption(image_path, max_length=50):
    features = extract_features(image_path)
    features = torch.tensor(features).unsqueeze(0)
    input_ids = tokenizer.encode("Image caption: ", return_tensors='pt')
    with torch.no_grad():
        outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)

    caption = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return caption
image_path = '/content/image.jpeg'
caption = generate_caption(image_path)
print("Generated Caption:", caption)
image = Image.open(image_path)
plt.imshow(image)
plt.axis('off')
plt.title(caption)
plt.show()
